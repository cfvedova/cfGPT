{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8579dc7-1c36-4d48-b46e-c57bb13ee57f",
   "metadata": {},
   "source": [
    "Test Trained Model\r\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a467e-10b7-40c9-a4e4-d7279a6514f8",
   "metadata": {},
   "source": [
    "### Import Data ###\n",
    "Some warnings are expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198d4e9-5349-4b2d-a445-36f5d7494c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.functional.regression import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f638d-e53a-4801-96da-2ac953eaeba8",
   "metadata": {},
   "source": [
    "### Hyperparameters ###\n",
    "MODIFY THESE FOR YOUR SPECIFIC TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8704e-84bd-4df4-8b88-c54070bddbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"save/scGPT_human\" # Base scGPT model that LOAD_MODEL_NAME was trained from\n",
    "DATASET_NAME_CONFIG = \"tabula_sapiens\" # Base dataset name\n",
    "LOAD_MODEL_NAME = \"save/tabula_sapiens-best-hvg\" # Model to be tested\n",
    "CELL_LABEL_DATASET_NAME = \"./Dataset/label_data.csv\" # Used to get output cellname list\n",
    "TEST_DATASET_NAME = './Dataset/arp4_cfrna_gene_counts.csv' # CSV Dataset you wish to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b978159-ba75-4e43-87a4-e82eadaa0ea0",
   "metadata": {},
   "source": [
    "### Default Hyperparameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a02bcf-cd59-41c1-a506-3a26c4e19567",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Further hyperparameters usually do not need to be modified\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### DEFAULT HYPERPARAMETERS ###\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=DATASET_NAME_CONFIG,\n",
    "    do_train=False,\n",
    "    load_model=LOAD_MODEL_NAME,\n",
    "    mask_ratio=0.0,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=16,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")\n",
    "config = hyperparameter_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8120b0-36a0-4e53-b52b-0941fed546ed",
   "metadata": {},
   "source": [
    "### Setup basic details ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b83e8-3f8e-44cd-abe0-87d499359a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config[\"seed\"])\n",
    "\n",
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config[\"mask_ratio\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config[\"include_zero_gene\"]  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config[\"n_bins\"]\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config[\"MVC\"]  # Masked value prediction for cell embedding\n",
    "ECS = config[\"ecs_thres\"] > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config[\"ecs_thres\"]\n",
    "dab_weight = config[\"dab_weight\"]\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config[\"lr\"]  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config[\"batch_size\"]\n",
    "eval_batch_size = config[\"batch_size\"]\n",
    "epochs = config[\"epochs\"]\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config[\"fast_transformer\"]\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config[\"layer_size\"]  # embedding dimension\n",
    "d_hid = config[\"layer_size\"]  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config[\"nlayers\"]  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config[\"nhead\"]  # number of heads in nn.MultiheadAttention\n",
    "dropout = config[\"dropout\"]  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config[\"save_eval_interval\"]  # epochs\n",
    "do_eval_scib_metrics = True\n",
    "\n",
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d8f8e-583e-46be-9dbd-f2d7a37d6ca7",
   "metadata": {},
   "source": [
    "### New Dataset Loader ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f49a3-1fa0-44f9-998a-c3833b411e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will change depending on our input dataset (ASK CONOR FOR QUESTIONS)\n",
    "# Important Data: gene_name (Will be set based on adata.var.index), X (Raw data values)\n",
    "# Un-important Data: batch_id, str_batch (Assigned based on train or test)\n",
    "data_dir = Path(f\"../data/{dataset_name}\")\n",
    "label_data = pd.read_csv(CELL_LABEL_DATASET_NAME, index_col=0)\n",
    "print(label_data)\n",
    "ori_batch_col = \"batch\"\n",
    "data_is_raw = True\n",
    "filter_gene_by_counts = False\n",
    "\n",
    "with open(TEST_DATASET_NAME) as test_data_file:\n",
    "    adata_test = ad.read_csv(test_data_file, first_column_names=True)\n",
    "\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"0\"\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels_test = adata_test.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata_test.obs[\"batch_id\"] = batch_id_labels_test\n",
    "\n",
    "num_types = len(np.unique(label_data.columns))\n",
    "celltypes_labels_names = np.unique(label_data.columns)\n",
    "adata_test.var[\"gene_name\"] = adata_test.var.index.tolist()\n",
    "\n",
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    base_model = Path(BASE_MODEL_NAME)\n",
    "    model_config_file = base_model / \"args.json\"\n",
    "    model_file = model_dir / \"model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata_test.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata_test.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata_test.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata_test = adata_test[:, adata_test.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    print(n_layers_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040dd2a-b102-4d29-bd40-17ae1b9ac8a1",
   "metadata": {},
   "source": [
    "### Preprocess Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b6e50-62d4-4bf6-ab1b-432a8804954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "# Filter genes, counts, normalization, binning, etc.\n",
    "preprocessor(adata_test, batch_key=None)\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "genes = adata_test.var[\"gene_name\"].tolist()\n",
    "\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd231ebe-9846-494b-8be7-feaf210a20bd",
   "metadata": {},
   "source": [
    "### Create Basic Dataset Classes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fc5eb-3e31-4816-ac41-9c7053f610b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "        print(f\"Number of workers: {num_workers}\")\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bbed9-546a-4ee5-9500-c1fbf6006da6",
   "metadata": {},
   "source": [
    "### Prepare Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bd526-b620-4d72-8bdf-3912fc55c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=1,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        if not torch.cuda.is_available():\n",
    "            pretrained_dict = torch.load(model_file, map_location=torch.device('cpu'))\n",
    "        else:\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.L1Loss() # Cell classification is now Cell proportion classification (This is a regression task, no longer classification)\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ae8ee-a37d-4ed9-8691-3d1b7b0617aa",
   "metadata": {},
   "source": [
    "### Functions to Test and Evaluate Model on Data ###\n",
    "Feel free to modify these as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4e881-8594-47ca-b246-3a756c5fb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "            \n",
    "            preds = output_values.cpu().numpy()\n",
    "            print(preds)\n",
    "            predictions.append(preds)\n",
    "    print(predictions)\n",
    "    print(num_types)\n",
    "    return np.concatenate(predictions, axis=0)\n",
    "\n",
    "### INFERENCE ###\n",
    "def test(model: nn.Module, adata: ad.AnnData) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader\n",
    "    )\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36614666-baf8-43cf-8638-764fce69441c",
   "metadata": {},
   "source": [
    "### Run actual Predictions and Create Graphs ###\n",
    "Actual testing part. All of the above is setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e977d8c-0448-4e4d-96c8-56a4603b04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test(model, adata_test)\n",
    "predictions = np.maximum(predictions, 0)[0] # Ensure no negative predictions\n",
    "results = {}\n",
    "print(f\"Model Predictions for celltype: {predictions}\")\n",
    "\n",
    "predictions = predictions / np.sum(predictions) # Normalize predictions\n",
    "celltypes_labels_names = celltypes_labels_names[predictions > 0.01] # Select top cell results\n",
    "predictions = predictions[predictions > 0.01] # Select top predictions\n",
    "\n",
    "# Present data as a pie chart\n",
    "explode = [0] * len(predictions)\n",
    "explode[np.argmax(predictions)] = 0.1\n",
    "\n",
    "palette_color = sns.color_palette('dark') \n",
    "\n",
    "# plotting data on chart \n",
    "# plotting data on chart \n",
    "plt.pie(predictions, labels=celltypes_labels_names, colors=palette_color, \n",
    "        explode=explode, autopct='%.0f%%') \n",
    "  \n",
    "# displaying chart \n",
    "plt.show()\n",
    "plt.savefig(save_dir / \"results.png\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
